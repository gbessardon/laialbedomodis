{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b239c1-e314-463e-b796-77e6ae78c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3157c2d-1743-4cf7-a89b-d4a39259eaeb",
   "metadata": {},
   "source": [
    "# Declare the MCD15A2H folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cd935d-3ada-4883-8590-c5b7e8daa333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collection dir is assumed to contain a folder of each year of the product\n",
    "\n",
    "\"\"\"\n",
    "OUTPUTDIR='tilesDIR'\n",
    "DATADIR='/home/gbessardon/DATA'\n",
    "SHORTNAME='MCD15A2H'\n",
    "COLLECTION='061'\n",
    "collectiondir=os.path.join(DATADIR,SHORTNAME,COLLECTION)\n",
    "years=['2020', '2021', '2018', '2019','2017']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f330a7a-41c0-42d7-abaf-7cbd38316185",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function to read the hdf file metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7edb6d-60ba-4c5a-a7a6-bb070068a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(ds):\n",
    "    geoT = ds.GetGeoTransform()\n",
    "    projT = ds.GetProjection()\n",
    "    w=ds.RasterXSize\n",
    "    h=ds.RasterYSize\n",
    "    offset=0\n",
    "    scale_factor=1\n",
    "    fillvalue=9999\n",
    "    for item,value in ds.GetMetadata_Dict().items():\n",
    "        if item=='add_offset':\n",
    "            offset=int(value)\n",
    "        if item=='scale_factor':\n",
    "            scale_factor=float(value)\n",
    "        if item=='_FillValue':\n",
    "            fillvalue=int(value)\n",
    "    return(offset,scale_factor,fillvalue,geoT,projT,w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829f940-20ea-465b-848e-667311841581",
   "metadata": {},
   "source": [
    "# Function to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098ad44-599e-46b7-976d-2093d67398d0",
   "metadata": {},
   "source": [
    "## Function to get the corresponding integer to the QC bit-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14571707-738a-4d3b-9e9c-9f7158409adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Getvalidqcinteger(bit57=['000','001'],bit34=['00','11'],\n",
    "                      bit2=['0'],bit1=['0','1'],bit0=['0']):\n",
    "    valbit=[]\n",
    "    valint=[]\n",
    "    for b57 in bit57:\n",
    "        for b34 in bit34:\n",
    "            for b2 in bit2:\n",
    "                for b1 in bit1:\n",
    "                    for b0 in bit0:\n",
    "                        bit=b57+b34+b2+b1+b0\n",
    "                        qc=int(bit,2)\n",
    "                        valbit.append(bit)\n",
    "                        valint.append(qc)\n",
    "    return(valbit,valint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826525c0-422b-4ded-b0d4-51ddf4b18596",
   "metadata": {},
   "source": [
    "## Function to apply the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d569304c-c4be-40c7-8562-38a04e3dc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(Ar,QCarray,maxvalue,validQCvalues,add_offset=0,scale_factor=1,fillvalue=9999,ECOSGscale_factor=100):\n",
    "    Arm=Ar*0\n",
    "    Armasked=Ar\n",
    "    Armasked[Ar>maxvalue]=fillvalue\n",
    "    for val in validQCvalues:\n",
    "        Arm[QCarray==val]=1\n",
    "    \n",
    "    Armasked[Arm==0]=fillvalue\n",
    "    Armasked = scale_factor * (Armasked - add_offset)*ECOSGscale_factor\n",
    "    new_fill=scale_factor*(fillvalue- add_offset)*ECOSGscale_factor\n",
    "    return(Armasked.astype(int),new_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a0938-c67d-41f6-af15-d082252486fd",
   "metadata": {},
   "source": [
    "## Gather all data treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cefdb55-6f63-4676-aa62-fdfe9e9eb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onefilefilter_treatment(fn,laiband=1,QCband=2,maxlai=100,fillvalue=255,ECOSGsf=100):\n",
    "    sds=gdal.Open(fn).GetSubDatasets()\n",
    "    dslai=gdal.Open(sds[laiband][0])\n",
    "    dsQC=gdal.Open(sds[QCband][0])\n",
    "    bites,QCv=Getvalidqcinteger()\n",
    "    of,sf,fil,geoT,projT,w,h=extract_metadata(dslai)\n",
    "    LAI=dslai.ReadAsArray()\n",
    "    QCar=dsQC.ReadAsArray()\n",
    "    LAImasked,filln=clean_data(LAI,QCar,maxlai,QCv,\n",
    "                               add_offset=of,scale_factor=sf,fillvalue=fil,ECOSGscale_factor=ECOSGsf)\n",
    "    return (LAImasked,filln,geoT,projT,w,h,of,sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54868edd-b7be-4518-ae7e-76c2ed3e3c23",
   "metadata": {},
   "source": [
    "# Functions to Create raster from numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d05de46-bd0b-4b0a-9f33-18bee7b7d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster(output_path,columns,rows,nband = 1,gdal_data_type = gdal.GDT_Int16, driver = r'GTiff'):\n",
    "    ''' returns gdal data source raster object\n",
    "\n",
    "    '''\n",
    "    # create driver\n",
    "    driver = gdal.GetDriverByName(driver)\n",
    "\n",
    "    output_raster = driver.Create(output_path,\n",
    "                                  int(columns),\n",
    "                                  int(rows),\n",
    "                                  nband,\n",
    "                                  eType = gdal_data_type)    \n",
    "    return output_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9522a73-b973-4688-9857-aba580ce578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_array_to_raster(output_path,\n",
    "                          numpy_array,\n",
    "                          geoTin,\n",
    "                          projectionin,\n",
    "                          nband = 1,\n",
    "                          no_data = -9999,\n",
    "                          gdal_data_type = gdal.GDT_Int16,\n",
    "                          driver = r'GTiff'):\n",
    "    ''' returns a gdal raster data source\n",
    "\n",
    "    keyword arguments:\n",
    "\n",
    "    output_path -- full path to the raster to be written to disk\n",
    "    numpy_array -- numpy array containing data to write to raster\n",
    "    geoTin -- geotransform input from ds.GetGetGeoTransform()\n",
    "    projectionin -- projection input from ds.GetProjection()\n",
    "    nband -- the band to write to in the output raster\n",
    "    no_data -- value in numpy array that should be treated as no data\n",
    "    gdal_data_type -- gdal data type of raster (see gdal documentation for list of values)\n",
    "    driver -- string value of the gdal driver to use\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    rows, columns = numpy_array.shape\n",
    "\n",
    "    # create output raster\n",
    "    output_raster = create_raster(output_path,\n",
    "                                  int(columns),\n",
    "                                  int(rows),\n",
    "                                  nband,\n",
    "                                  gdal_data_type) \n",
    "\n",
    "    geotransform = geoTin\n",
    "\n",
    "\n",
    "    output_raster.SetProjection(projectionin)\n",
    "    output_raster.SetGeoTransform(geotransform)\n",
    "    output_band = output_raster.GetRasterBand(1)\n",
    "    output_band.SetNoDataValue(no_data)\n",
    "    output_band.WriteArray(numpy_array)          \n",
    "    output_band.FlushCache()\n",
    "    output_band.ComputeStatistics(False)\n",
    "\n",
    "    if os.path.exists(output_path) == False:\n",
    "        raise Exception('Failed to create raster: %s' % output_path)\n",
    "\n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd009e-409c-44e9-8123-c4a9eccfc14f",
   "metadata": {},
   "source": [
    "# Get the list of all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7b51a3-43bf-48de-b09d-1d7860c4eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_filelist(collectiondir,years):\n",
    "    filelistMCD15A2H=[]\n",
    "    for y in years:\n",
    "        if os.path.exists(os.path.join(collectiondir,y)) == True:\n",
    "            for f in os.listdir(os.path.join(collectiondir,y)):\n",
    "                filelistMCD15A2H.append(os.path.join(collectiondir,y,f))\n",
    "    return (filelistMCD15A2H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91108a23-9a8b-4a80-9004-dfd556f05afd",
   "metadata": {},
   "source": [
    "## Get the list of tiles in the existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4dcdc5-56f5-49d0-88e5-6ae2a948318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_tilelist(filelist,tilepattern='.h\\d\\dv\\d\\d'):\n",
    "    tilelist= [re.search(tilepattern,f).group() for f in filelist]\n",
    "    hlist=[int(t.split('h')[1].split('v')[0]) for t in tilelist]\n",
    "    vlist=[int(t.split('h')[1].split('v')[1]) for t in tilelist]\n",
    "    return(tilelist,vlist,hlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c030884-0ea3-4941-8174-d88cc3f76f60",
   "metadata": {},
   "source": [
    "## Get the day list in the MODIS files by identifying the pattern AYYYYJJJ then gathering then in the ECOSG 10-days classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54c4d2d-3b0c-4813-a754-4fc9d6bc3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_10dayslist(filelist,dayspattern='.A\\d\\d\\d\\d\\d\\d\\d',\n",
    "                     ECOSGdates=['0105', '0115', '0125', '0205', '0215', '0225', '0305', '0315',\n",
    "                                 '0325', '0405', '0415', '0425', '0505', '0515', '0525', '0605',\n",
    "                                 '0615', '0625', '0705', '0715', '0725', '0805', '0815', '0825',\n",
    "                                 '0905', '0915', '0925', '1005', '1015', '1025', '1105', '1115',\n",
    "                                 '1125', '1205', '1215', '1225']):\n",
    "    dayslist= [re.search(dayspattern,f).group() for f in filelist]\n",
    "    yearlist=[int(d[2:6]) for d in dayslist]\n",
    "    juliandays=[int(d[6:9]) for d in dayslist]\n",
    "    julian10daysclass=[int((j/365)*36) for j in juliandays]\n",
    "    ECOSG10days=[ECOSGdates[j10] for j10 in julian10daysclass]\n",
    "    return(yearlist,juliandays,julian10daysclass,ECOSG10days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fc4ef-42ba-4e11-bb1d-c9cb012b834d",
   "metadata": {},
   "source": [
    "## Gather the previous function and extract the data in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4912c847-3f77-4511-add0-c53aa8ca0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_dataframe(collectiondir,years):\n",
    "    filelist=Create_filelist(collectiondir,years)\n",
    "    tilelist,vlist,hlist=Create_tilelist(filelist)\n",
    "    yearlist,juliandays,julian10daysclass,ECOSG10days=Create_10dayslist(filelist)\n",
    "    data = {'filename':filelist, 'tile':tilelist,'htile':hlist,'vtile':vlist,\n",
    "            'year':yearlist,'julianday':juliandays,'julian10days':julian10daysclass,\n",
    "            'ecosg10':ECOSG10days}\n",
    "    df=pd.DataFrame(data)\n",
    "    groupeddftilejuliandays=df.groupby(['htile','vtile','ecosg10'])\n",
    "    return(df,groupeddftilejuliandays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9df31-be3d-4cb6-9799-1d9c4e02ca53",
   "metadata": {},
   "source": [
    "## Function to create the output dir if it does not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0578b6-5ada-4b6f-8bd6-54016d065396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Createoutputdir(outputdir):\n",
    "    outpath=os.path.join(os.getcwd(),outputdir)\n",
    "    if not os.path.isdir(outpath):\n",
    "        os.mkdir(outpath)\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22deefe9-ae82-433e-a186-a35dd9504876",
   "metadata": {},
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440f188-4ce6-44b8-8aa7-6af8476fe1b4",
   "metadata": {},
   "source": [
    "## Create dataframe with the different files available and sort them by tile and 10days period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a9994f-0543-427f-bcba-d1150607918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,groupeddftilejuliandays=Create_dataframe(collectiondir,years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f600613-efd1-4176-abd4-7785bc0778eb",
   "metadata": {},
   "source": [
    "## Create output directory if does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "528bfad0-9508-4a99-ad78-4de80213f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath=Createoutputdir(OUTPUTDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08911412-1cd2-49a9-9d3c-49abc092c8ec",
   "metadata": {},
   "source": [
    "## Loop over the group aplly quality filter and calculate the 10-day mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a84448e-8f76-47fe-899d-82fa613602f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 2, '0715')\n",
      "(11, 2, '0805')\n",
      "(11, 2, '0825')\n",
      "(12, 1, '0715')\n",
      "(12, 1, '0725')\n",
      "(12, 1, '0805')\n",
      "(12, 1, '0815')\n",
      "(12, 1, '0825')\n",
      "(12, 2, '0625')\n",
      "(12, 2, '0715')\n",
      "(12, 2, '0725')\n",
      "(12, 2, '0805')\n",
      "(12, 2, '0825')\n",
      "(13, 1, '0715')\n",
      "(13, 1, '0725')\n",
      "(13, 1, '0805')\n",
      "(13, 1, '0815')\n",
      "(13, 1, '0825')\n",
      "(13, 2, '0625')\n",
      "(13, 2, '0725')\n",
      "(13, 2, '0815')\n",
      "(13, 2, '0825')\n",
      "(14, 1, '0715')\n",
      "(14, 1, '0725')\n",
      "(14, 1, '0805')\n",
      "(14, 1, '0815')\n",
      "(14, 1, '0825')\n",
      "(14, 2, '0625')\n",
      "(14, 2, '0715')\n",
      "(14, 2, '0725')\n",
      "(14, 2, '0805')\n",
      "(14, 2, '0815')\n",
      "(15, 1, '0715')\n",
      "(15, 1, '0725')\n",
      "(15, 1, '0805')\n",
      "(15, 1, '0815')\n",
      "(15, 1, '0825')\n",
      "(15, 2, '0715')\n",
      "(15, 2, '0725')\n",
      "(15, 2, '0805')\n",
      "(15, 2, '0815')\n",
      "(15, 2, '0825')\n",
      "(16, 0, '0715')\n",
      "(16, 0, '0725')\n",
      "(16, 0, '0805')\n",
      "(16, 0, '0815')\n",
      "(16, 0, '0825')\n",
      "(16, 1, '0715')\n",
      "(16, 1, '0725')\n",
      "(16, 1, '0805')\n",
      "(16, 1, '0815')\n",
      "(16, 1, '0825')\n",
      "(16, 2, '0625')\n",
      "(16, 2, '0715')\n",
      "(16, 2, '0725')\n",
      "(16, 2, '0805')\n",
      "(16, 2, '0815')\n",
      "(16, 2, '0825')\n",
      "(17, 0, '0715')\n",
      "(17, 0, '0725')\n",
      "(17, 0, '0805')\n",
      "(17, 0, '0815')\n",
      "(17, 0, '0825')\n",
      "(17, 1, '0625')\n",
      "(17, 1, '0715')\n",
      "(17, 1, '0725')\n",
      "(17, 1, '0805')\n",
      "(17, 1, '0815')\n",
      "(17, 1, '0825')\n",
      "(17, 2, '0625')\n",
      "(17, 2, '0715')\n",
      "(17, 2, '0725')\n",
      "(17, 2, '0805')\n",
      "(17, 2, '0815')\n",
      "(17, 2, '0825')\n",
      "(18, 0, '0715')\n",
      "(18, 0, '0725')\n",
      "(18, 0, '0805')\n",
      "(18, 0, '0815')\n",
      "(18, 0, '0825')\n",
      "(18, 1, '0625')\n",
      "(18, 1, '0715')\n",
      "(18, 1, '0725')\n",
      "(18, 1, '0805')\n",
      "(18, 1, '0815')\n",
      "(18, 1, '0825')\n",
      "(18, 2, '0715')\n",
      "(18, 2, '0725')\n",
      "(18, 2, '0805')\n",
      "(18, 2, '0815')\n",
      "(18, 2, '0825')\n",
      "(19, 0, '0715')\n",
      "(19, 0, '0725')\n",
      "(19, 0, '0805')\n",
      "(19, 0, '0815')\n",
      "(19, 1, '0705')\n",
      "(19, 1, '0715')\n",
      "(19, 1, '0725')\n",
      "(19, 1, '0805')\n",
      "(19, 1, '0815')\n",
      "(19, 1, '0825')\n",
      "(19, 2, '0705')\n",
      "(19, 2, '0715')\n",
      "(19, 2, '0725')\n",
      "(19, 2, '0805')\n",
      "(19, 2, '0815')\n",
      "(19, 2, '0825')\n",
      "(20, 1, '0715')\n",
      "(20, 1, '0725')\n",
      "(20, 1, '0815')\n",
      "(20, 2, '0715')\n",
      "(20, 2, '0725')\n",
      "(20, 2, '0815')\n",
      "(21, 1, '0715')\n",
      "(21, 1, '0725')\n",
      "(21, 1, '0805')\n",
      "(21, 1, '0815')\n",
      "(21, 1, '0825')\n",
      "(21, 2, '0715')\n",
      "(21, 2, '0725')\n",
      "(21, 2, '0815')\n",
      "(22, 1, '0715')\n",
      "(22, 1, '0725')\n",
      "(22, 1, '0805')\n",
      "(22, 1, '0815')\n",
      "(22, 1, '0825')\n",
      "(22, 2, '0705')\n",
      "(22, 2, '0715')\n",
      "(22, 2, '0725')\n",
      "(22, 2, '0805')\n",
      "(23, 1, '0625')\n",
      "(23, 1, '0715')\n",
      "(23, 1, '0725')\n",
      "(23, 1, '0805')\n",
      "(23, 1, '0815')\n",
      "(23, 1, '0825')\n",
      "(23, 2, '0705')\n",
      "(23, 2, '0715')\n",
      "(23, 2, '0725')\n",
      "(23, 2, '0805')\n",
      "(23, 2, '0815')\n",
      "(23, 2, '0825')\n",
      "(24, 2, '0715')\n",
      "(24, 2, '0805')\n",
      "(24, 2, '0815')\n",
      "(24, 2, '0825')\n"
     ]
    }
   ],
   "source": [
    "files_issuelist=[]\n",
    "for name, group in groupeddftilejuliandays:\n",
    "        LAIlist=[]\n",
    "        print(name)\n",
    "        for i,fn in enumerate(group.filename):\n",
    "            try:\n",
    "                (LAImasked,filln,geoT,projT,w,h,of,sf)=onefilefilter_treatment(fn)\n",
    "                LAIlist.append(np.ma.masked_array(LAImasked,LAImasked==filln))\n",
    "            except:\n",
    "                files_issuelist.append(fn)\n",
    "        LAI_median=np.ma.median(np.ma.stack(LAIlist),axis=0,overwrite_input=True)\n",
    "        numpy_array_to_raster(os.path.join(outpath,'LAI'+str(name[0])+'v'+str(name[1])+'jd'+str(name[2])+'.tif'),\n",
    "                              LAI_median,geoT,projT,nband = 1,no_data = filln,gdal_data_type = gdal.GDT_Int16,driver = r'GTiff')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
